{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "071257d8",
   "metadata": {},
   "source": [
    "# UT Austin CS 370 Undergraduate Research Project Spring 2022\n",
    "## Researcher: Tejas Saboo <br> Supervisor: Professor Angela Beasley"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf54b7bd",
   "metadata": {},
   "source": [
    "# Machine Learning Model for the Stylistic Classification of Fine Art (Binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98afb296",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "Emerging technologies are disrupting the art industry and changing the way we create and experience art. As a computer science, mathematics, and statistics student with a deep appreciation for arts and culture, I am fascinated by how technology shapes our society. One recent technological advancement I am excited about is OpenAI's Dall-E 2, an AI system that can create realistic images and art from a description in natural language. I enjoy learning about innovative inventions and discoveries, and I love applying my learnings to build impactful technologies. For my undergraduate research project, I decided to explore the intersection of art, design, and technology to identify and address opportunities to augment human creativity. The project entailed architecting a deep convolutional neural network model to classify visual fine artwork by its artistic style.\n",
    "\n",
    "### Results\n",
    "The final machine learning model presented is a deep convolutional neural network for the binary classification of fina art by its artistic style. The style of an artwork refers to its distinctive visual elements, techniques, and methods, typically corresponding with an art movement or school group that the artist is associated with. The model below can be used for any binary classification problem, and the overal model architecture can be used for multi-class classification with minor modifications. Currently, the model is used to distinguish between the artistic styles of Realism and Pointillism. The model is highly successful with a 95.6250% accuracy on the training set and 64.5000% testing accuracy on the test set after 78 epochs of training.\n",
    "\n",
    "\n",
    "### Experimental Process\n",
    "I experimented with a variety of different technical approaches and model architectures throughout this project. First, I reviewed neural networks and machine learning concepts, learned about computer vision, and determined the relevant libraries. I decided that using Python's PyTorch library would be ideal for creating a customizable and scalable model for image classification tasks. Then, I scoped out my project and established a timeline of major milestones. Next, I searched for art databases and found a dataset with images of flower and marina paintings. After finding a relevant dataset, I set up a custom data loader using the torchvision library's ImageFolder and torch library's DataLoader functions. Then, I performed a manual train-test split. For each stylistic category, I moved 80 images into the training set and 20 images into the test set. Next, I designed a simple linear neural network model, created a training function, and implemented an evaluation function. I trained the model on the flower and marina painting dataset, then performed hyperparameter tuning. Then, I realized that the model was too simple, so I experimented with various architectures such as linear and non-linear neural networks, convolutional neural networks, fully convolutional neural networks, and deep convolutional neural networks. By limiting the size of the initial train and test sets, I was able to quickly train the models and continuously prototype the model architecture. Next, I searched for a dataset with artwork categorized by its artistic style, and I discovered the Wikiart database and found a corresponding Wikiart dataset that I decided to use for my final model. I updated the model to be compatible with the wikiart dataset and retrained it. However, I realized that manually moving images from the wikiart directory to the train and test directories would be tedious and impractical at scale, so I decided to automate the process. I learned how to use the shutil.move functionality to automate moving files from one directory to another, resulting in a scalable train-test split that also randomly shuffles the images. This allowed me to have train and test datasets on the order of hundreds and thousands of images in seconds. Then, I retrained the model on the larger dataset for a binary classification problem. Noting the low training accuracy, I updated the model to use blocks. While this improved the training accuracy significantly, I now had the problem of overfitting since there was a significant disparity between the training and testing accuracies. I implemented data augmentation in the training image transform function, experimented with various criterion loss functions, tested stochastic gradient descent versus adam for the optimizer, investigated regularization techniques, added dropouts, and tried various learning rates throughout my hyperparameter tuning process. This successfully reduced overfitting and improved model train and test accuracy. Then, I modified the model for multi-class image classification tasks. Finally, I modularized my functions, added documentation, and made final improvements. \n",
    "\n",
    "### Future Plans\n",
    "My future plans to build on this project include experimenting with computer vision to extract visual features and elements of the artwork such as line, shape, texture, form, space, color, value, composition, perspective, and subject matter to create algorithms that suggest tags for artwork. Next, I will create an algorithm that generates original and relevant titles for the artwork using the tags and extracted visual features. I will also investigate recent artificial intelligence and machine learning advancements to create an algorithm that measures the creativity and craftsmanship expemplified in the artwork and provides actionable feedback to help artists improve. Furthermore, I plan on learning more about blockchain and NFTs and possibly launching my own project.\n",
    "\n",
    "### Involvement in the Scientific Community\n",
    "Throughout the semester, I made a conscious effort to become more involved and engaged with the scientific community. My first step was to learn and contribute to academia through this undergraduate research project. Next, I read research publications and articles about technological advancements in industry. I decided to take this a step further by attending research conferences and symposiums, faculty talks, industry events, and honors thesis defenses. These include:\n",
    "<br>\n",
    "- Institute for Foundations of Machine Learning: AI for Accurate and Fair Imaging with Alex Dimakis\n",
    "<br>\n",
    "- Good Systems Annual Symposium: Defining, Evaluating, and Building Ethical AI Systems\n",
    "<br>\n",
    "- 2022 UT Machine Learning Laboratory Research Symposium\n",
    "<br>\n",
    "- Quantum Computing Triple Play at UT Austin (Quantum Conference with Dr. Aaronson, Atom Computing, and Strangeworks)\n",
    "<br>\n",
    "- Tesla Cyber Rodeo\n",
    "<br> \n",
    "- Chat with Jahmy Hindman, the CTO of John Deere\n",
    "<br>\n",
    "- Bloomberg Engineering: Inclusive Branch (Re)Naming at Scale\n",
    "<br>\n",
    "- Hosting UT Entrepreneurship Week and Moderating the Democratizing Healthcare Innovation Panel\n",
    "<br>\n",
    "- Undergraduate Honors Thesis Defense\n",
    "<br>\n",
    "- Quantum Information Science Final Presentations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff1748d",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "Downloadable Dataset Link: https://drive.google.com/file/d/182-pFiKvXPB25DbTfAYjJ6gDE-ZCRXz0/view\n",
    "\n",
    "The dataset used for this model is scraped from Wikiart.org, a visual art encyclopedia. The dataset has several images corresponding to various artistic styles. In the dataset, there are several sub-directories. The name of each sub-directory is the artistic style, and every image of a painting within those sub-directories belong to that given artistic style category. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d947860c",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "238e7f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for moving files and ML functionality\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef19f4d",
   "metadata": {},
   "source": [
    "### Functions to Automate the Train-Test Dataset Split\n",
    "\n",
    "The move_shuffled_files function moves a random subset of files from the source directory to the destination directory.\n",
    "\n",
    "The train_test_split function automates the process of moving files from the original wikiart directory to the train and test directories for every relevant artistic style category. I decided to use an 80:20 split because a research publication by Gholamy et al., Why 70/30 or 80/20 Relation Between Training and Testing Sets: A Pedagogical Explanation, found that empirical evidence suggests that using 80% of data for training and the remaining 20% of data for testing causes the best results for smaller datasets (Gholamy et al., 2018). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9845108d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly selects num_files images from the src directory and moves them to the dest directory\n",
    "def move_shuffled_files(src, dest, num_files):\n",
    "    # Get the list of all files in the src directory\n",
    "    files = os.listdir(src)\n",
    "    # Select a random sample of num_files from files and move them to the dest directory\n",
    "    for file_name in random.sample(files, num_files):\n",
    "        shutil.move(os.path.join(src, file_name), dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fdfd862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automates the train-test split to move images from the wikiart dataset to corresponding train and test directories\n",
    "def train_test_split(wiki_dirs, train_size, test_size):\n",
    "    # Initialize the base directories\n",
    "    base_src = 'wikiart/'\n",
    "    base_train_dest = 'data/train/'\n",
    "    base_test_dest = 'data/test/'\n",
    "    # Perform a train-test split for each desired art category\n",
    "    for wiki_dir in wiki_dirs:\n",
    "        # Get the src for the current art category\n",
    "        src = base_src + wiki_dir\n",
    "        # Get the train and test dests for the current art category\n",
    "        train_dest = base_train_dest + wiki_dir\n",
    "        test_dest = base_test_dest + wiki_dir\n",
    "        # Perform a train-test split by moving files from src to the train and test dest directories\n",
    "        move_shuffled_files(src, train_dest, train_size)\n",
    "        move_shuffled_files(src, test_dest, test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81b32866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# art_wiki_directories is the set of desired art categories to classify \n",
    "art_wiki_directories = ['Pointillism', 'Realism']\n",
    "# Perform a 80:20 train-test split on the desired art categories \n",
    "train_test_split(art_wiki_directories, 400, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2144cf5c",
   "metadata": {},
   "source": [
    "### Train and Test Transform Functions To Set Up Images for the Model\n",
    "\n",
    "The train and test transform functions resize the images to 256 x 256 x 3 and convert them to tensors. This allows the data to be efficiently computed in the neural network. I experimented with several sizes ranging from 32 x 32 x 3 to 512 x 512 x 3, and I found that 256 x 256 x 3 resulted in the optimal time-accuracy trade off. Furthermore, I implemented data augmentation through the RandomHorizontalFlip, ColorJitter, and RandomGrayscale transformations in the train transform function to reduce overfitting. During hyperparameter tuning, I also tested several different probabilities for the parameters of these functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb4e0fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform function for the training data with data augmentation to reduce overfitting\n",
    "train_transform = transforms.Compose([\n",
    "                                      transforms.Resize([256, 256]),\n",
    "                                      transforms.RandomHorizontalFlip(p=0.4),\n",
    "                                      transforms.ColorJitter(0.5,0.5,0.5,0.5),\n",
    "                                      transforms.RandomGrayscale(0.10),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize(mean=(0.5,0.5,0.5), std=(0.5,0.5,0.5))\n",
    "])\n",
    "\n",
    "# Transform function for the test data\n",
    "test_transform = transforms.Compose([\n",
    "                                      transforms.Resize([256, 256]),\n",
    "                                      transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba88238",
   "metadata": {},
   "source": [
    "### Generate Train and Test Sets\n",
    "\n",
    "The following 2 cells generate the train and test sets for the model. They utilize the above transformation functions and some library functionality. The datasets.ImageFolder method in torchvision's library facilitates the process of accessing images from desired folders while maintaining information about their corresponding class labels. The PyTorch DataLoader creates an iterable dataset designed specifically for neural networks. Shuffle is set to true in the train data loader to permute the indices of all samples to prevent overfitting. Additionally, a batch size of 4 was selected through hyperparameter tuning to reduce memory overhead and overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee11b474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the train set using ImageFolder and DataLoader\n",
    "train_data = datasets.ImageFolder(root=\"data/train\", transform=train_transform)\n",
    "train = DataLoader(train_data, batch_size = 4, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d49d19c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the test set using ImageFolder and DataLoader\n",
    "test_data = datasets.ImageFolder(root=\"data/test\", transform=test_transform)\n",
    "test = DataLoader(test_data, batch_size = 4, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d777ebc",
   "metadata": {},
   "source": [
    "### Deep Convolutional Neural Network\n",
    "\n",
    "I created a deep convolutional neural network for the fine art classification task. This model is ideal because it provides an efficient and dense network with a strong performance in image classification. The deep convolutional neural network architecture helps the model detect low-level features in the initial layers and high-level features in the subsequent layers, resulting in a robust model with the ability to classify complex images. The model accomplishes this by taking advantage of the 2D structure of input images using local connections and tied weights followed by polling to translate invariant features. Furthermore, this architecture allows the model to have fewer parameters than fully connected networks with the same number of hidden units, resulting in faster training. Additionally, my design leverages blocks, which are components with multiple layers, to increase model complexity without limiting readability. The model has 3 blocks, and each block has a series of batch normalizations, activation functions, convolutonal layers, and max pooling. Finally, I employ several strategies to reduce overfitting, such as the use of data augmentation in the train transform function, applying batch normalization and max pooling, and using dropouts. I also performed hyperparameter tuning on the model architecture and number of blocks, activation functions, dropout probabilities, data agumentation transformation functions and their probabilities, batch sizes, image resize dimensions, learning rate, criterion loss functions, and stochastic gradient descent versus adam optimizers, among others. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3b370e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block structure for the neural network\n",
    "class CNNBlock(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, conv_kernel, conv_stride, conv_padding, pool_kernel, pool_stride):\n",
    "        super().__init__()\n",
    "        # L is the set of layers for the block\n",
    "        L = [\n",
    "            # Applies batch normalization on the input channels\n",
    "            torch.nn.BatchNorm2d(in_channels),\n",
    "            # Applies the rectified linear unit activation function\n",
    "            torch.nn.ReLU(),\n",
    "            # Applies a 2D convolutional layer\n",
    "            torch.nn.Conv2d(in_channels, out_channels, conv_kernel, conv_stride, conv_padding, dilation=1, \n",
    "                            groups=1, bias=True, padding_mode='zeros', device=None, dtype=None),\n",
    "            # Applies batch normalization on the output channels\n",
    "            torch.nn.BatchNorm2d(out_channels),\n",
    "            # Applies randomized leaky rectified linear unit activation function\n",
    "            torch.nn.RReLU(0.1, 0.25),\n",
    "            # Applies a 2D max pooling over an input signal composed of several input planes\n",
    "            torch.nn.MaxPool2d(pool_kernel, pool_stride),\n",
    "        ]\n",
    "        # self.network holds a sequential container of the layers defined above\n",
    "        self.network = torch.nn.Sequential(*L)\n",
    "    \n",
    "    # Perform forward propagation on the block\n",
    "    def forward(self, x):\n",
    "        # Forward propagate the input through the sequential layers\n",
    "        return self.network(x)\n",
    "\n",
    "# Deep Convolutional Neural Network Classifier Model for Binary Classification \n",
    "class DeepCNNClassifier(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # in_channels is the number of channels in the input image\n",
    "        in_channels = 3\n",
    "        # out_channels is the number of channels produced by the convolution\n",
    "        out_channels = 32\n",
    "        # num_classes is the number of art categories being classified\n",
    "        num_classes = 2\n",
    "        # kernel_size is the size of the filter mask window\n",
    "        kernel_size = 7\n",
    "        # stride is the size of the movement of the mask window over the image\n",
    "        stride = 1\n",
    "        # padding is the amount of pixels being added to the image\n",
    "        padding = 3\n",
    "        # L is the set of layers for the CNN\n",
    "        L = [\n",
    "            # Add a CNN Block\n",
    "            CNNBlock(in_channels, out_channels, kernel_size, stride, padding, 2, 2),\n",
    "            # Randomly zeroes elements of the input tensor with 0.2 probability to reduce overfitting\n",
    "            torch.nn.Dropout(p=0.25),\n",
    "            # Add a CNN Block\n",
    "            CNNBlock(32, 64, 3, 1, 1, 2, 2),\n",
    "            # Randomly zeroes elements of the input tensor with 0.25 probability to reduce overfitting\n",
    "            torch.nn.Dropout(p=0.25),\n",
    "            # Add a CNN Block\n",
    "            CNNBlock(64, 128, 3, 1, 1, 2, 2),\n",
    "            # Applies a 2D max pooling over an input signal composed of several input planes\n",
    "            torch.nn.MaxPool2d(2),\n",
    "            # Creates a feature vector by flattening data into a 1D tensor \n",
    "            torch.nn.Flatten(),\n",
    "            # Applies a linear transformation for classification\n",
    "            torch.nn.Linear(32768, num_classes)\n",
    "        ]\n",
    "        # self.network holds a sequential container of the layers defined above\n",
    "        self.network = torch.nn.Sequential(*L)\n",
    "        # self.transforms holds a transform function to normalize data\n",
    "        self.transforms = torchvision.transforms.Compose(\n",
    "            [torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "    # Perform forward propagation on the CNN\n",
    "    def forward(self, x):\n",
    "        # Apply transform to normalize input\n",
    "        x = self.transforms(x)\n",
    "        # Forward propagate the input through the sequential layers\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d336eda7",
   "metadata": {},
   "source": [
    "### Set Up the Model\n",
    "\n",
    "The below cell initializes the model and its criterion loss function and optimizer for back propagation. The cross entropy loss function computes the cross entropy loss between the input and target. The adam optimizer implements the Adam method, which is a computationally efficient algorithm for first-order gradient-descent based optimization of stochastic objective functions based on adaptive estimates of lower-order movements. Extensive hyperparameter tuning resulted in the use of cross entropy loss as the criterion, Adam as the optimizer, and 0.001 as the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33310fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initalize the Deep Convolutional Neural Network Model\n",
    "model = DeepCNNClassifier()\n",
    "# Criterion loss function for training the neural network\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Optimizer for modifying weights during back propagation\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr = 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06ff712",
   "metadata": {},
   "source": [
    "### Set Up Training Functionality\n",
    "\n",
    "The train_iter function trains the model for one epoch, and the train_model function runs the train_iter function for the desired number of epochs in a loop. Training the model entails getting predictions from the model for all training input images, computing the loss between the predicted label and the true label for all training images, and performing back propagation to fine-tune and update the model weights to improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6863703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model for one epoch\n",
    "def train_iter(epoch, net, trainDataLoader, optimizer, criterion):\n",
    "    # Set the model in training mode\n",
    "    net.train()\n",
    "    # Initialize counters\n",
    "    total = 0\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    # Train on every image input and target classification label pair from the train set\n",
    "    for inputs, targets in trainDataLoader:\n",
    "        # Set the optimizer to zero grad to make it non-cumulative by zeroing out all gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Get predictions from the model for the inputs\n",
    "        outputs = net(inputs)\n",
    "        # Get the (value, index) tuples for the maximum values across the 1st dimension of outputs\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        # Compute the loss which is the distance of the prediction from the true label\n",
    "        loss = criterion(outputs, targets)\n",
    "        # Perform back propagation to update weights\n",
    "        loss.backward()\n",
    "        # Gradient descent\n",
    "        optimizer.step()\n",
    "        # Update loss, total, and correct counters\n",
    "        train_loss += loss.item()\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "    \n",
    "    # Compute and print the average loss and accuracy\n",
    "    avg_loss = train_loss / len(trainDataLoader)\n",
    "    accuracy = 100 * correct / total\n",
    "    print('Epoch: {} \\tTraining Loss: {:.25f} \\tTraining Accuracy: {:.4f} %'.format(epoch, avg_loss, accuracy))\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "51646581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model for the desired number of epochs, num_epochs\n",
    "def train_model(num_epochs, net, trainDataLoader, optimizer, criterion):\n",
    "  # Call the train_iter function for every epoch\n",
    "  for i in range(num_epochs):\n",
    "    loss = train_iter(i, net, trainDataLoader, optimizer, criterion)\n",
    "    # Early stopping to avoid overfitting and stop training after reaching desired training accuracy\n",
    "    if loss <= 0.09:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8c6eeb",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b24939c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tTraining Loss: 5.6309458738429203350506214 \tTraining Accuracy: 54.5000 %\n",
      "Epoch: 1 \tTraining Loss: 4.2583660170020447566230359 \tTraining Accuracy: 57.0000 %\n",
      "Epoch: 2 \tTraining Loss: 3.8397715035622241863677573 \tTraining Accuracy: 56.3750 %\n",
      "Epoch: 3 \tTraining Loss: 3.1513164115115070984529666 \tTraining Accuracy: 57.7500 %\n",
      "Epoch: 4 \tTraining Loss: 2.3312036786042154012932315 \tTraining Accuracy: 58.3750 %\n",
      "Epoch: 5 \tTraining Loss: 2.1762228657188824598733845 \tTraining Accuracy: 61.0000 %\n",
      "Epoch: 6 \tTraining Loss: 1.5818134208209813085233009 \tTraining Accuracy: 63.1250 %\n",
      "Epoch: 7 \tTraining Loss: 1.1275966120511293766526251 \tTraining Accuracy: 61.7500 %\n",
      "Epoch: 8 \tTraining Loss: 0.8075523518584668547504180 \tTraining Accuracy: 65.0000 %\n",
      "Epoch: 9 \tTraining Loss: 0.7332977724447846679112217 \tTraining Accuracy: 65.2500 %\n",
      "Epoch: 10 \tTraining Loss: 0.7358261621743440450416074 \tTraining Accuracy: 64.6250 %\n",
      "Epoch: 11 \tTraining Loss: 0.6813935177214444083304556 \tTraining Accuracy: 66.1250 %\n",
      "Epoch: 12 \tTraining Loss: 0.6321207023132592306424726 \tTraining Accuracy: 69.5000 %\n",
      "Epoch: 13 \tTraining Loss: 0.6729277708195149365622001 \tTraining Accuracy: 68.8750 %\n",
      "Epoch: 14 \tTraining Loss: 0.6609285553917289002257007 \tTraining Accuracy: 66.7500 %\n",
      "Epoch: 15 \tTraining Loss: 0.6116298331692814338467201 \tTraining Accuracy: 71.0000 %\n",
      "Epoch: 16 \tTraining Loss: 0.6327093587629497584146065 \tTraining Accuracy: 68.2500 %\n",
      "Epoch: 17 \tTraining Loss: 0.6329623118788003655055263 \tTraining Accuracy: 70.1250 %\n",
      "Epoch: 18 \tTraining Loss: 0.6414055224880576000501264 \tTraining Accuracy: 71.5000 %\n",
      "Epoch: 19 \tTraining Loss: 0.6299835350643843145945766 \tTraining Accuracy: 69.7500 %\n",
      "Epoch: 20 \tTraining Loss: 0.6400540071912109718610395 \tTraining Accuracy: 70.8750 %\n",
      "Epoch: 21 \tTraining Loss: 0.6237077738158405271207130 \tTraining Accuracy: 71.8750 %\n",
      "Epoch: 22 \tTraining Loss: 0.6129858725238591654616016 \tTraining Accuracy: 71.2500 %\n",
      "Epoch: 23 \tTraining Loss: 0.5142334107309579627198559 \tTraining Accuracy: 76.7500 %\n",
      "Epoch: 24 \tTraining Loss: 0.5505031982623040942570469 \tTraining Accuracy: 74.2500 %\n",
      "Epoch: 25 \tTraining Loss: 0.5324273638427257093752587 \tTraining Accuracy: 75.7500 %\n",
      "Epoch: 26 \tTraining Loss: 0.5566113136708736863766944 \tTraining Accuracy: 74.0000 %\n",
      "Epoch: 27 \tTraining Loss: 0.5799494408816099344505801 \tTraining Accuracy: 73.5000 %\n",
      "Epoch: 28 \tTraining Loss: 0.5519488201290368989404556 \tTraining Accuracy: 75.6250 %\n",
      "Epoch: 29 \tTraining Loss: 0.4934617718588560908443696 \tTraining Accuracy: 76.0000 %\n",
      "Epoch: 30 \tTraining Loss: 0.5527825928013772038838169 \tTraining Accuracy: 75.2500 %\n",
      "Epoch: 31 \tTraining Loss: 0.4861269211024045744373723 \tTraining Accuracy: 78.0000 %\n",
      "Epoch: 32 \tTraining Loss: 0.5005532200727611513713100 \tTraining Accuracy: 77.1250 %\n",
      "Epoch: 33 \tTraining Loss: 0.4811048775655217268898411 \tTraining Accuracy: 77.3750 %\n",
      "Epoch: 34 \tTraining Loss: 0.5175565494131296739865888 \tTraining Accuracy: 77.2500 %\n",
      "Epoch: 35 \tTraining Loss: 0.5003636165801436108679923 \tTraining Accuracy: 78.3750 %\n",
      "Epoch: 36 \tTraining Loss: 0.4471565682999789625995390 \tTraining Accuracy: 80.0000 %\n",
      "Epoch: 37 \tTraining Loss: 0.4301491365674883216030366 \tTraining Accuracy: 79.8750 %\n",
      "Epoch: 38 \tTraining Loss: 0.4941491729672998212130608 \tTraining Accuracy: 78.2500 %\n",
      "Epoch: 39 \tTraining Loss: 0.3739019628195092148637002 \tTraining Accuracy: 83.1250 %\n",
      "Epoch: 40 \tTraining Loss: 0.4509936781227588675768914 \tTraining Accuracy: 80.1250 %\n",
      "Epoch: 41 \tTraining Loss: 0.3911939849867485663459377 \tTraining Accuracy: 82.0000 %\n",
      "Epoch: 42 \tTraining Loss: 0.3958307011635042726993561 \tTraining Accuracy: 83.6250 %\n",
      "Epoch: 43 \tTraining Loss: 0.3688637592573650314520251 \tTraining Accuracy: 83.6250 %\n",
      "Epoch: 44 \tTraining Loss: 0.4005634080048184886280183 \tTraining Accuracy: 82.3750 %\n",
      "Epoch: 45 \tTraining Loss: 0.3930903300642967224121094 \tTraining Accuracy: 82.8750 %\n",
      "Epoch: 46 \tTraining Loss: 0.4019412455894053048943704 \tTraining Accuracy: 82.1250 %\n",
      "Epoch: 47 \tTraining Loss: 0.3494129199930466889156833 \tTraining Accuracy: 83.3750 %\n",
      "Epoch: 48 \tTraining Loss: 0.3608411134663038200010021 \tTraining Accuracy: 86.1250 %\n",
      "Epoch: 49 \tTraining Loss: 0.3490961084677837722445304 \tTraining Accuracy: 86.0000 %\n",
      "Epoch: 50 \tTraining Loss: 0.3501972998096607447848783 \tTraining Accuracy: 84.8750 %\n",
      "Epoch: 51 \tTraining Loss: 0.3059846842411206857548223 \tTraining Accuracy: 87.1250 %\n",
      "Epoch: 52 \tTraining Loss: 0.2829748723609373239895604 \tTraining Accuracy: 88.2500 %\n",
      "Epoch: 53 \tTraining Loss: 0.2763056481024250277123144 \tTraining Accuracy: 87.8750 %\n",
      "Epoch: 54 \tTraining Loss: 0.3008844628726364822668415 \tTraining Accuracy: 88.1250 %\n",
      "Epoch: 55 \tTraining Loss: 0.3152027779730269818614374 \tTraining Accuracy: 87.0000 %\n",
      "Epoch: 56 \tTraining Loss: 0.2724761654960456969121196 \tTraining Accuracy: 88.2500 %\n",
      "Epoch: 57 \tTraining Loss: 0.2787866711954120613192742 \tTraining Accuracy: 88.7500 %\n",
      "Epoch: 58 \tTraining Loss: 0.2724880105629563509239688 \tTraining Accuracy: 88.7500 %\n",
      "Epoch: 59 \tTraining Loss: 0.2548596916859969252477924 \tTraining Accuracy: 88.8750 %\n",
      "Epoch: 60 \tTraining Loss: 0.2540969144165864945783539 \tTraining Accuracy: 91.0000 %\n",
      "Epoch: 61 \tTraining Loss: 0.2747486904449760736213193 \tTraining Accuracy: 88.6250 %\n",
      "Epoch: 62 \tTraining Loss: 0.2025496128213126190154014 \tTraining Accuracy: 92.1250 %\n",
      "Epoch: 63 \tTraining Loss: 0.2414681115196435634562278 \tTraining Accuracy: 89.5000 %\n",
      "Epoch: 64 \tTraining Loss: 0.2115412992521305546311794 \tTraining Accuracy: 90.6250 %\n",
      "Epoch: 65 \tTraining Loss: 0.2306589703747886355156282 \tTraining Accuracy: 91.0000 %\n",
      "Epoch: 66 \tTraining Loss: 0.1923840202967403434364257 \tTraining Accuracy: 92.1250 %\n",
      "Epoch: 67 \tTraining Loss: 0.1977203077313606505160948 \tTraining Accuracy: 92.0000 %\n",
      "Epoch: 68 \tTraining Loss: 0.1801200975407846183173888 \tTraining Accuracy: 93.1250 %\n",
      "Epoch: 69 \tTraining Loss: 0.1829828020152854117785779 \tTraining Accuracy: 91.6250 %\n",
      "Epoch: 70 \tTraining Loss: 0.1977298813726520165801048 \tTraining Accuracy: 92.3750 %\n",
      "Epoch: 71 \tTraining Loss: 0.1888901360823365405128271 \tTraining Accuracy: 92.5000 %\n",
      "Epoch: 72 \tTraining Loss: 0.1922861988014483436426616 \tTraining Accuracy: 93.0000 %\n",
      "Epoch: 73 \tTraining Loss: 0.1653825663686439018107421 \tTraining Accuracy: 93.5000 %\n",
      "Epoch: 74 \tTraining Loss: 0.1644968301501648910711850 \tTraining Accuracy: 93.1250 %\n",
      "Epoch: 75 \tTraining Loss: 0.1481542465042002820752032 \tTraining Accuracy: 94.1250 %\n",
      "Epoch: 76 \tTraining Loss: 0.1713394066634646162317068 \tTraining Accuracy: 92.8750 %\n",
      "Epoch: 77 \tTraining Loss: 0.1281028492571022192603181 \tTraining Accuracy: 95.6250 %\n"
     ]
    }
   ],
   "source": [
    "# Train the model for 200 epochs\n",
    "train_model(200, model, train, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be06cb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tTraining Loss: 0.1565938093552540488762048 \tTraining Accuracy: 94.0000 %\n",
      "Epoch: 1 \tTraining Loss: 0.1574475397374771878933331 \tTraining Accuracy: 94.8750 %\n",
      "Epoch: 2 \tTraining Loss: 0.1298319004188852043846225 \tTraining Accuracy: 95.2500 %\n",
      "Epoch: 3 \tTraining Loss: 0.1248135358348372442316432 \tTraining Accuracy: 94.7500 %\n",
      "Epoch: 4 \tTraining Loss: 0.1390257454140191861746700 \tTraining Accuracy: 94.7500 %\n",
      "Epoch: 5 \tTraining Loss: 0.1224570002198379359281688 \tTraining Accuracy: 95.0000 %\n",
      "Epoch: 6 \tTraining Loss: 0.0965325004262922448106110 \tTraining Accuracy: 96.2500 %\n"
     ]
    }
   ],
   "source": [
    "train_model(200, model, train, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1469e574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tTraining Loss: 0.1510384704946045597573345 \tTraining Accuracy: 95.1250 %\n",
      "Epoch: 1 \tTraining Loss: 0.1054103614982341285966072 \tTraining Accuracy: 95.3750 %\n",
      "Epoch: 2 \tTraining Loss: 0.1367237270246368929793590 \tTraining Accuracy: 94.7500 %\n",
      "Epoch: 3 \tTraining Loss: 0.1065218708094289451482339 \tTraining Accuracy: 96.3750 %\n",
      "Epoch: 4 \tTraining Loss: 0.1062285400455948575215714 \tTraining Accuracy: 96.0000 %\n",
      "Epoch: 5 \tTraining Loss: 0.1070277983496271123620147 \tTraining Accuracy: 95.7500 %\n",
      "Epoch: 6 \tTraining Loss: 0.0712101295394029359941968 \tTraining Accuracy: 98.0000 %\n"
     ]
    }
   ],
   "source": [
    "train_model(10, model, train, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "144b61bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tTraining Loss: 0.1110185373912418033137683 \tTraining Accuracy: 95.5000 %\n"
     ]
    }
   ],
   "source": [
    "train_model(1, model, train, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b67f92",
   "metadata": {},
   "source": [
    "### Evaluate the Model on the Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77088bd7",
   "metadata": {},
   "source": [
    "After training and tuning the model, evaluate it by computing its accuracy on the test set. For every image in the test set, get its prediction and compare it to its true label to validate its correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "19e6c933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the accuracy of the model on the test set to evaluate its performance\n",
    "def eval_model(net, testDataLoader):\n",
    "    # Set the model in evaluation mode\n",
    "    net.eval()\n",
    "    # Initialize the counters\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # Disable gradient descent calculation for evaluation\n",
    "    with torch.no_grad():\n",
    "        # Evaluate the model against the test set\n",
    "        for inputs, targets in testDataLoader:\n",
    "            # Get the predictions from the model on the inputs\n",
    "            outputs = net(inputs)\n",
    "            # Get the (value, index) tuples for the maximum values across the 1st dimension of outputs\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            # Update the total count\n",
    "            total += len(targets)\n",
    "            # Compute and update the number of correct predictions by comparing against the target label\n",
    "            correct += (predicted == targets).sum().item()\n",
    "    \n",
    "    # Compute and print the accuracy of the model on the test set\n",
    "    test_acc = 100 * correct / total\n",
    "    print('Test Accuracy: {:.4f} %'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "45f91106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 61.5000 %\n"
     ]
    }
   ],
   "source": [
    "eval_model(model, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb3c565",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
